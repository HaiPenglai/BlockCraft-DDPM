# --- START OF FILE generate.py ---
import os
import json
import torch
import torch.nn as nn
from diffusers import UNet2DConditionModel, DDPMScheduler
from PIL import Image
from tqdm.auto import tqdm
import numpy as np

# --- 1. é…ç½®å‚æ•° ---
# æŒ‡å‘ä½ æƒ³è¯»å–çš„ epoch æ¨¡å‹è·¯å¾„ (æ¯”å¦‚ epoch_200 æ•ˆæœæœ€å¥½ï¼Œå°±æ”¹æˆ 200)
EPOCH_TO_LOAD = 200
MODEL_PATH = f"mc_blocks_ddpm_cond/epoch_{EPOCH_TO_LOAD:03d}/model.pt" 
MAPPING_PATH = "mc_blocks_ddpm_cond/class_mapping.json"
OUTPUT_DIR = "final_results"

IMAGE_SIZE = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
GUIDANCE_SCALE = 3.5 # CFG å¼•å¯¼å°ºåº¦ (é€šå¸¸ 3.0 ~ 7.0 æ•ˆæœæœ€ä½³ï¼Œè¶Šå¤§è¶Šè´´åˆæ¡ä»¶)
INFERENCE_STEPS = 50 # é‡‡æ ·æ­¥æ•°

@torch.no_grad()
def generate():
    if not os.path.exists(OUTPUT_DIR): 
        os.makedirs(OUTPUT_DIR)

    # 1. åŠ è½½ç±»åˆ«æ˜ å°„è¡¨
    if not os.path.exists(MAPPING_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç±»åˆ«æ˜ å°„æ–‡ä»¶: {MAPPING_PATH}ã€‚è¯·ç¡®ä¿è®­ç»ƒå·²æ­£å¸¸å¯åŠ¨è¿‡ã€‚")
    with open(MAPPING_PATH, "r", encoding="utf-8") as f:
        class_to_id = json.load(f)
    print("æ”¯æŒçš„æ–¹å—ç§ç±»:", list(class_to_id.keys()))

    # 2. åŠ è½½æƒé‡
    print(f"\nâ³ æ­£åœ¨ä» {MODEL_PATH} åŠ è½½æ¨¡å‹...")
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}ã€‚")
    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
    num_classes = checkpoint['num_classes']
    
    # åˆå§‹åŒ– UNet
    model = UNet2DConditionModel(
        sample_size=IMAGE_SIZE,
        in_channels=3,
        out_channels=3,
        layers_per_block=2,
        block_out_channels=(128, 256, 256, 512, 512),
        down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"),
        up_block_types=("UpBlock2D", "CrossAttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D"),
        cross_attention_dim=128,
    )
    model.load_state_dict(checkpoint['unet'])
    model.to(DEVICE).eval()

    # åˆå§‹åŒ– Embedding å±‚ (+1 æ˜¯ä¸ºäº†åŒ…å« CFG çš„æ— æ¡ä»¶ Token)
    label_emb = nn.Embedding(num_classes + 1, 128)
    label_emb.load_state_dict(checkpoint['emb'])
    label_emb.to(DEVICE).eval()

    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)

    # 3. æŒ‡å®šä½ æƒ³ç”Ÿæˆçš„æ–¹å—åˆ—è¡¨
    # ä½ å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹ä½ æƒ³ç”Ÿæˆçš„æ–¹å—ï¼Œæ¯”å¦‚æˆ‘è¦ç”Ÿæˆ 1ä¸ªé’»çŸ³ã€1ä¸ªçº¢ç –å—
    tasks = [
        "diamond_ore", # é’»çŸ³çŸ¿
        "emerald_block", # ç»¿å®çŸ³å—
        "brick", # çº¢ç –å—
        "cobblestone", # åœ†çŸ³
        "planks_oak", # æ©¡æœ¨æ¿
        "stone",            # çŸ³å¤´
        "dirt",             # æ³¥åœŸ
        "sand",             # æ²™å­
        "gravel",           # æ²™ç ¾
        "bedrock",          # åŸºå²©
        "iron_ore",         # é“çŸ¿çŸ³
        "gold_ore",         # é‡‘çŸ¿çŸ³
        "coal_ore",         # ç…¤çŸ¿çŸ³
        "lapis_block",      # é’é‡‘çŸ³å—
        "obsidian",         # é»‘æ›œçŸ³
        "glass",            # ç»ç’ƒ (é€æ˜ç‰¹å¾å­¦ä¹ )
        "stonebrick",       # çŸ³ç –
        "netherrack",       # ä¸‹ç•Œå²©
        "bookshelf",        # ä¹¦æ¶ (çº¹ç†è¾ƒå¤æ‚ï¼Œé€‚åˆæŒ‘æˆ˜æ¨¡å‹)
        "glowstone",        # è¤çŸ³ (é«˜äº®åº¦å’Œå¤æ‚çº¹ç†)
    ]

    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆå›¾ç‰‡ (CFG Scale: {GUIDANCE_SCALE}, æ­¥æ•°: {INFERENCE_STEPS})")
    noise_scheduler.set_timesteps(INFERENCE_STEPS)

    for block_name in tasks:
        if block_name not in class_to_id:
            print(f"âš ï¸ è­¦å‘Š: ç±»åˆ« '{block_name}' ä¸åœ¨è®­ç»ƒé›†ç±»åˆ«ä¸­ï¼Œå·²è·³è¿‡ã€‚")
            continue
            
        print(f"æ­£åœ¨ç”Ÿæˆ: {block_name} ...")
        
        # å‡†å¤‡ CFG çš„æ¡ä»¶ Embedding å’Œæ— æ¡ä»¶ Embedding
        target_id = class_to_id[block_name]
        cond_id = torch.tensor([target_id], device=DEVICE)
        uncond_id = torch.tensor([num_classes], device=DEVICE) # ä½¿ç”¨ç¬¬ num_classes ä½œä¸ºç©ºæ ‡ç­¾
        
        cond_emb = label_emb(cond_id).unsqueeze(1)    # [1, 1, 128]
        uncond_emb = label_emb(uncond_id).unsqueeze(1) # [1, 1, 128]

        # åˆå§‹éšæœºå™ªå£°
        image = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)

        # é‡‡æ ·å¾ªç¯
        for t in tqdm(noise_scheduler.timesteps, leave=False):
            # å°† latent å’Œ emb å‡å¤åˆ¶æˆ 2 ä»½ï¼Œå‰åŠéƒ¨åˆ†ç»™ unconditionalï¼ŒååŠéƒ¨åˆ†ç»™ conditional
            latent_model_input = torch.cat([image] * 2)
            emb_input = torch.cat([uncond_emb, cond_emb])
            
            # é¢„æµ‹å™ªå£°
            noise_pred = model(latent_model_input, t, encoder_hidden_states=emb_input).sample
            
            # æ‰§è¡Œ CFG (æ— åˆ†ç±»å™¨å¼•å¯¼)
            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)
            noise_pred_cfg = noise_pred_uncond + GUIDANCE_SCALE * (noise_pred_cond - noise_pred_uncond)
            
            # æ­¥è¿›
            image = noise_scheduler.step(noise_pred_cfg, t, image).prev_sample

        # 4. åå¤„ç†å¹¶ä¿å­˜
        image = (image / 2 + 0.5).clamp(0, 1) # è¿˜åŸåˆ° [0, 1]
        image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
        image = (image * 255).astype(np.uint8)
        
        save_name = f"result_{block_name}.png"
        Image.fromarray(image).save(os.path.join(OUTPUT_DIR, save_name))
        print(f"âœ… ä¿å­˜æˆåŠŸ: {OUTPUT_DIR}/{save_name}")

    print("\nğŸ‰ å…¨éƒ¨ç”Ÿæˆå®Œæ¯•ï¼")

if __name__ == "__main__":
    generate()
# --- END OF FILE generate.py ---