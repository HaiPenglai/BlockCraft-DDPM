# --- START OF FILE train.py ---
import os
import json
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from accelerate import Accelerator
from diffusers import UNet2DConditionModel, DDPMScheduler
from diffusers.optimization import get_cosine_schedule_with_warmup
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from tqdm.auto import tqdm

# --- é…ç½® ---
class Config:
    dataset_dir = "dataset"
    output_dir = "mc_blocks_ddpm_cond" # ç±»ä¼¼å››å¶è‰çš„å‘½åé£æ ¼
    image_size = 128
    
    # è®­ç»ƒå‚æ•°
    train_batch_size = 64  # RTX 5090 æ˜¾å­˜éå¸¸å¤§ï¼Œ64æˆ–128éƒ½å¯ä»¥è½»æ¾åº”å¯¹
    learning_rate = 1e-4
    num_epochs = 200       # éµå¾ªä½ ä¹‹å‰æˆåŠŸçš„200è½®ç»éªŒ
    save_every_epochs = 10 # æ¯10è½®ä¿å­˜å¹¶æµ‹è¯•
    mixed_precision = "bf16" # RTX 5090 æ”¯æŒ bf16ï¼Œé€Ÿåº¦æ›´å¿«
    num_workers = 8

    # CFG (Classifier-Free Guidance) è®­ç»ƒé…ç½®
    cfg_drop_rate = 0.1    # 10% çš„æ¦‚ç‡ä¸¢å¼ƒæ¡ä»¶ï¼Œç”¨äºè®­ç»ƒæ— æ¡ä»¶ç”Ÿæˆèƒ½åŠ›
    cross_attention_dim = 128

# --- æ•°æ®é›†å¤„ç† (åŠ¨æ€è¯»å–ç›®å½•) ---
class MCBlockDataset(Dataset):
    def __init__(self, root_dir, transform):
        self.samples = []
        self.transform = transform
        
        # åŠ¨æ€è·å–æ‰€æœ‰ç±»åˆ«
        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
        self.class_to_id = {cls_name: i for i, cls_name in enumerate(self.classes)}
        
        for cls_name in self.classes:
            folder_path = os.path.join(root_dir, cls_name)
            for img_name in os.listdir(folder_path):
                if img_name.lower().endswith(('.png', '.jpg')):
                    self.samples.append({
                        "path": os.path.join(folder_path, img_name),
                        "label_id": self.class_to_id[cls_name]
                    })
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œå…± {len(self.classes)} ä¸ªç±»åˆ«ï¼Œ{len(self.samples)} å¼ å›¾ç‰‡ã€‚")

    def __len__(self): return len(self.samples)

    def __getitem__(self, i):
        sample = self.samples[i]
        image = Image.open(sample["path"]).convert("RGB")
        return self.transform(image), torch.tensor(sample["label_id"], dtype=torch.long)

# --- è®­ç»ƒé€»è¾‘ ---
def train():
    config = Config()
    accelerator = Accelerator(mixed_precision=config.mixed_precision)

    # 1. æ•°æ®é›†å‡†å¤‡
    preprocess = transforms.Compose([
        transforms.Resize((config.image_size, config.image_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ])
    dataset = MCBlockDataset(config.dataset_dir, preprocess)
    train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_workers)
    num_classes = len(dataset.classes)

    # ä¿å­˜ç±»åˆ«æ˜ å°„ï¼Œä¾› generate.py ä½¿ç”¨
    if accelerator.is_main_process:
        os.makedirs(config.output_dir, exist_ok=True)
        os.makedirs("samples_mc", exist_ok=True)
        with open(os.path.join(config.output_dir, "class_mapping.json"), "w", encoding="utf-8") as f:
            json.dump(dataset.class_to_id, f, ensure_ascii=False, indent=4)

    # 2. å®šä¹‰æœ‰æ¡ä»¶ UNet
    model = UNet2DConditionModel(
        sample_size=config.image_size,
        in_channels=3,
        out_channels=3,
        layers_per_block=2,
        block_out_channels=(128, 256, 256, 512, 512),
        down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"),
        up_block_types=("UpBlock2D", "CrossAttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D"),
        cross_attention_dim=config.cross_attention_dim, 
    )
    
    # ğŸŒŸ æ ¸å¿ƒè¦æ±‚ï¼šå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œæ‹¯æ•‘/ä¼˜åŒ–æ˜¾å­˜
    model.enable_gradient_checkpointing() 

    # 3. è¯­ä¹‰ Embedding å±‚ (é‡ç‚¹ï¼šå¤šåŠ 1ä¸ªIDç»™â€œç©ºæ¡ä»¶â€ä½œä¸ºCFGçš„æ— æ¡ä»¶å¼•å¯¼)
    # index 0 ~ num_classes-1 æ˜¯çœŸå®ç±»åˆ«ï¼Œindex = num_classes æ˜¯æ— æ¡ä»¶(uncond)
    label_emb = nn.Embedding(num_classes + 1, config.cross_attention_dim) 

    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
    optimizer = torch.optim.AdamW(list(model.parameters()) + list(label_emb.parameters()), lr=config.learning_rate)

    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=500,
        num_training_steps=(len(train_dataloader) * config.num_epochs),
    )

    model, label_emb, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, label_emb, optimizer, train_dataloader, lr_scheduler
    )

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.num_epochs):
        model.train()
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch+1}/{config.num_epochs}")
        
        for step, (images, labels) in enumerate(train_dataloader):
            # --- CFG è®­ç»ƒæ ¸å¿ƒé€»è¾‘ ---
            # æœ‰ config.cfg_drop_rate çš„æ¦‚ç‡ï¼ŒæŠŠåŸæœ¬çš„æ ‡ç­¾æ›¿æ¢ä¸º num_classes (å³æ— æ¡ä»¶ç±»åˆ«)
            drop_mask = torch.rand(labels.shape, device=labels.device) < config.cfg_drop_rate
            cfg_labels = torch.where(drop_mask, torch.tensor(num_classes, device=labels.device), labels)
            
            # UNet2DConditionModel éœ€è¦çš„ç»´åº¦æ˜¯ (BS, Seq_Len, Dim)ï¼Œå› æ­¤å¢åŠ ä¸€ä¸ª Seq_Len=1 çš„ç»´åº¦
            encoder_hidden_states = label_emb(cfg_labels).unsqueeze(1) # Shape: [BS, 1, 128]

            noise = torch.randn_like(images)
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (images.shape[0],), device=images.device).long()
            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)

            # é¢„æµ‹å™ªå£°
            noise_pred = model(noisy_images, timesteps, encoder_hidden_states=encoder_hidden_states).sample
            loss = F.mse_loss(noise_pred, noise)
            
            accelerator.backward(loss)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()

            progress_bar.update(1)
            progress_bar.set_postfix(loss=loss.item())

        # ğŸŒŸ æ¯ 10 è½®ä¿å­˜ä¸ã€æµ‹è¯•é‡‡æ ·ã€‘
        if (epoch + 1) % config.save_every_epochs == 0 and accelerator.is_main_process:
            model.eval()
            print(f"\nâœ¨ Epoch {epoch+1}: æ­£åœ¨è¿›è¡Œ CFG æ¡ä»¶ç”Ÿæˆæµ‹è¯•...")
            
            # éšæœºæŠ½2ä¸ªç±»åˆ«æµ‹è¯•
            test_classes = random.sample(dataset.classes, 2)
            
            with torch.no_grad():
                for test_cls in test_classes:
                    target_id = dataset.class_to_id[test_cls]
                    
                    # å‡†å¤‡ CFG çš„æ¡ä»¶å’Œæ— æ¡ä»¶ Embedding
                    cond_id = torch.tensor([target_id], device=accelerator.device)
                    uncond_id = torch.tensor([num_classes], device=accelerator.device) # ç©ºæ¡ä»¶
                    
                    cond_emb = label_emb(cond_id).unsqueeze(1)    # [1, 1, 128]
                    uncond_emb = label_emb(uncond_id).unsqueeze(1) # [1, 1, 128]
                    
                    # é‡‡æ ·èµ·ç‚¹
                    sample = torch.randn(1, 3, config.image_size, config.image_size).to(accelerator.device)
                    
                    # ä¸ºäº†æµ‹è¯•é€Ÿåº¦å¿«ï¼Œè®¾ç½® 50 æ­¥é‡‡æ ·
                    noise_scheduler.set_timesteps(50)
                    guidance_scale = 3.0 # CFG å¼•å¯¼ç³»æ•°
                    
                    for t in tqdm(noise_scheduler.timesteps, leave=False, desc=f"Generating {test_cls}"):
                        # CFG æ¨ç†ï¼šåŒæ—¶è®¡ç®—æ¡ä»¶å’Œæ— æ¡ä»¶çš„å™ªå£°é¢„æµ‹ (åˆå¹¶ä¸ºä¸€ä¸ª Batch æé«˜æ•ˆç‡)
                        latent_model_input = torch.cat([sample] * 2)
                        emb_input = torch.cat([uncond_emb, cond_emb])
                        
                        noise_pred = model(latent_model_input, t, encoder_hidden_states=emb_input).sample
                        
                        # æ‹†åˆ†é¢„æµ‹ç»“æœå¹¶è¿ç”¨ CFG å…¬å¼
                        noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)
                        noise_pred_cfg = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)
                        
                        sample = noise_scheduler.step(noise_pred_cfg, t, sample).prev_sample
                    
                    # ä¿å­˜å›¾ç‰‡
                    sample_img = ((sample / 2 + 0.5).clamp(0, 1) * 255).permute(0, 2, 3, 1).cpu().numpy().astype("uint8")[0]
                    Image.fromarray(sample_img).save(f"samples_mc/epoch_{epoch+1:03d}_{test_cls}.png")
            
            # ä¿å­˜è¯¥è½®çš„æ¨¡å‹
            checkpoint_dir = os.path.join(config.output_dir, f"epoch_{epoch+1:03d}")
            os.makedirs(checkpoint_dir, exist_ok=True)
            torch.save({
                'unet': accelerator.unwrap_model(model).state_dict(),
                'emb': accelerator.unwrap_model(label_emb).state_dict(),
                'num_classes': num_classes
            }, os.path.join(checkpoint_dir, "model.pt"))
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {checkpoint_dir}")

if __name__ == "__main__":
    train()
# --- END OF FILE train.py ---